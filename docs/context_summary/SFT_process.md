# Agent SFT Data Processing & Training Pipeline

This directory contains the complete pipeline for processing Agent data and training models with summary-enhanced supervised fine-tuning (SFT).

## Overview

The pipeline transforms raw Agent traces into two specialized training datasets that enable models to both generate summaries and perform efficient reasoning.

## ðŸ“ Data Processing Pipeline

### Original Data
- **File**: `filtered_results_sample_200.jsonl`
- **Content**: 200 Agent reasoning traces with multi-step search and reasoning processes

## Data Processing Pipeline

### Process Raw Data into Multi-Turn Format

```bash
python3 examples/data_preprocess/process_search_agent_multiturn.py \
    --input_file data/sft_compress/filtered_results_sample_200.jsonl \
    --output_file data/sft_compress/sft_train_multiturn.jsonl \
    --output_format jsonl \
    --think_drop_prob 0.3
```


### Dataset with Summary


**Script**: `examples/data_preprocess/add_summary_multiturn.py`

**Usage**:
```bash
# ä½¿ç”¨OpenAI APIæ·»åŠ summaryï¼ˆéœ€è¦API keyï¼‰
python3 examples/data_preprocess/add_summary_multiturn.py \
    --input_file data/sft_compress/sft_train_multiturn.jsonl \
    --output_file data/sft_compress/sft_train_multiturn_with_summary.jsonl \
    --max_concurrent 100
```


**Summary Components**:
1. **Question**: Clearly state what needs to be answered
2. **Reasoning Summary**: Brief description of logical thinking process  
3. **Search Results Summary**: Key information from search results (verbatim quotes)



3. Dataset Splitting: Create two specialized datasets for different training objectives.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature             â”‚ Dataset 1 (Prev Turn)                        â”‚ Dataset 2 (Summary Only) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Prompt Source       â”‚ Question + previous summary/action + latest obs â”‚ Summary only           â”‚
â”‚ Answer Source       â”‚ Summary + Original answer                     â”‚ Original answer          â”‚
â”‚ Avg Token Length    â”‚ ~1700 tokens                                  â”‚ ~1500 tokens             â”‚
â”‚ Learning Objective  â”‚ Summarize & plan the next turn                â”‚ Efficient reasoning      â”‚
â”‚ Training Speed      â”‚ Moderate                                      â”‚ Faster (shorter seq)     â”‚
â”‚ Dataset Format      â”‚ MultiTurnSFTDataset (complete conversations)  â”‚ MultiTurnSFTDataset      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Usage**:
```bash
bash examples/data_preprocess/run_multiturn_pipeline.sh
```


## ðŸš€ Training Scripts

### Progressive Training (Three-Phase)

**Script**: `examples/sft/context_summary/train_progressive.sh`

**Three-Phase Training**:

```
Phase 1: Multi-turn SFT (Complete Reasoning Chains)
â”œâ”€ Dataset: sft_train_multiturn.parquet
â”œâ”€ Duration: 10 epochs
â”œâ”€ Learning Rate: 1e-5
â”œâ”€ Goal: Learn complete multi-turn reasoning chains
â””â”€ Output: phase1_multiturn/global_step_*

         â†“ (Use Phase 1 checkpoint as initialization)

Phase 2: Summary Only (Efficient Reasoning)
â”œâ”€ Dataset: sft_train_summary_only.parquet
â”œâ”€ Duration: 10 epochs
â”œâ”€ Learning Rate: 1e-5
â”œâ”€ Goal: Learn efficient reasoning from summaries
â””â”€ Output: phase2_summary_only/global_step_*

         â†“ (Use Phase 2 checkpoint as initialization)

Phase 3: Prev-Turn Context (Summary Generation)
â”œâ”€ Dataset: sft_train_summary_prefix.parquet
â”œâ”€ Duration: 5 epochs
â”œâ”€ Learning Rate: 5e-6 (lower to preserve previous learning)
â”œâ”€ Goal: Add summary generation capability
â””â”€ Output: phase3_summary_prefix/global_step_* (final model)
```


**Usage**:
```bash
# Basic progressive training (8 GPUs)
bash examples/sft/context_summary/train_progressive.sh 8 /tmp/sft_progressive
```

**Training Flow**:
1. Phase 1 trains on `sft_train_multiturn.parquet` (complete reasoning chains)
2. Script automatically finds latest Phase 1 checkpoint
3. Phase 2 uses Phase 1 checkpoint as `model.partial_pretrain` and trains on `sft_train_summary_only.parquet`
4. Script automatically finds latest Phase 2 checkpoint
5. Phase 3 uses Phase 2 checkpoint as `model.partial_pretrain` and trains on `sft_train_summary_prefix.parquet`
6. Final model saved in `phase3_summary_prefix/global_step_*`

**Recommended for evaluation**: Use Phase 3 final checkpoint for inference and downstream tasks.

## ðŸ“Š Training Parameters

## ðŸ” Evaluation

**Script**: `examples/sft/context_summary/evaluate_model.py`

**Manual Evaluation**:
```bash
python3 examples/sft/context_summary/evaluate_model.py \
    --model_path /tmp/sft_mixed/phase2_summary_prefix/global_step_50 \
    --test_file data/sft_compress/sft_train_with_summary.jsonl \
    --output results/eval_results.json \
    --num_samples 50 \
    --max_new_tokens 1024
```

## ðŸŽ¯ Quick Start

**Complete pipeline from scratch**:

```bash
# 1. Process traces into multiturn format
python3 examples/data_preprocess/process_search_agent_multiturn.py \
    --input_file data/sft_compress/filtered_results_sample_200.jsonl \
    --output_file data/sft_compress/sft_train_multiturn.jsonl \
    --output_format jsonl \
    --think_drop_prob 0.3

# 2. Add summaries to multiturn data (only turn_index >= 1)
python3 examples/data_preprocess/add_summary_multiturn.py \
    --input_file data/sft_compress/sft_train_multiturn.jsonl \
    --output_file data/sft_compress/sft_train_multiturn_with_summary.jsonl \
    --max_concurrent 100

# 3. Create Dataset 1 & 2, Split train/val and convert to Parquet
bash examples/data_preprocess/run_multiturn_pipeline.sh

# 4. Train with progressive strategy (recommended)
bash examples/sft/context_summary/train_progressive.sh 8 verl_checkpoints/sft_progressive

```
