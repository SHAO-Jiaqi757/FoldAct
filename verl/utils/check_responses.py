"""
ç®€æ´çš„ responses å’Œ response_mask æ£€æŸ¥å·¥å…·
"""

import torch
from typing import Dict, Optional
from verl import DataProto


def check_responses_and_mask(
    batch: DataProto,
    sample_idx: int = 0,
    verbose: bool = True,
    tokenizer=None
) -> Dict[str, bool]:
    """
    æ£€æŸ¥ responses å’Œ response_mask çš„æ­£ç¡®æ€§
    
    Args:
        batch: æ•°æ®æ‰¹æ¬¡
        sample_idx: é‡‡æ ·æ£€æŸ¥çš„æ ·æœ¬ç´¢å¼•
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
    Returns:
        æ£€æŸ¥ç»“æœå­—å…¸ï¼ŒåŒ…å«å„é¡¹æ£€æŸ¥çš„é€šè¿‡çŠ¶æ€
    """
    results = {
        "responses_exists": False,
        "response_mask_exists": False,
        "shapes_match": False,
        "mask_valid": False,
        "responses_in_range": False,
        "all_passed": False,
    }
    
    # å¼ºåˆ¶è¾“å‡º batch keysï¼ˆå¸®åŠ©è°ƒè¯•ï¼‰
    print(f"Batch keys: {list(batch.batch.keys())}")
    
    if "responses" not in batch.batch:
        print("âŒ responses not found in batch")
        print(f"Available keys: {list(batch.batch.keys())}")
        return results
    
    responses = batch.batch["responses"]
    results["responses_exists"] = True
    
    # å¼ºåˆ¶è¾“å‡ºåŸºæœ¬ä¿¡æ¯ï¼ˆå³ä½¿ verbose=Falseï¼‰
    print(f"ğŸ“Š Responses check:")
    print(f"  Shape: {responses.shape}")
    print(f"  Dtype: {responses.dtype}")
    try:
        print(f"  Min: {responses.min().item()}, Max: {responses.max().item()}")
    except Exception as e:
        print(f"  âš ï¸  Failed to get min/max: {e}")
    
    # æ£€æŸ¥å€¼åŸŸï¼ˆtoken IDs åº”è¯¥æ˜¯éè´Ÿæ•´æ•°ï¼‰
    if responses.min() >= 0:
        results["responses_in_range"] = True
    elif verbose:
        print(f"  âš ï¸  WARNING: Negative token IDs found!")
    
    # æ£€æŸ¥ response_mask
    if "response_mask" in batch.batch:
        response_mask = batch.batch["response_mask"]
        results["response_mask_exists"] = True
        
        # å¼ºåˆ¶è¾“å‡ºåŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“Š Response mask check:")
        print(f"  Shape: {response_mask.shape}")
        print(f"  Dtype: {response_mask.dtype}")
        
        # æ£€æŸ¥ mask æ¥æºå’Œæ–¹æ³•
        mask_source = "unknown"
        if "responses_types" in batch.batch:
            mask_source = "responses_types (multi-turn aware)"
        elif "info_mask" in batch.batch:
            mask_source = "info_mask"
        elif "attention_mask" in batch.batch:
            mask_source = "attention_mask"
        
        print(f"\n  ğŸ” Mask source: {mask_source}")
        
        # å¯¹æ¯” info_mask å’Œ attention_maskï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "info_mask" in batch.batch and "attention_mask" in batch.batch:
            info_mask_full = batch.batch["info_mask"]
            attention_mask_full = batch.batch["attention_mask"]
            response_length = responses.shape[1]
            
            if info_mask_full.shape[1] >= response_length and attention_mask_full.shape[1] >= response_length:
                info_mask_response = info_mask_full[:, -response_length:]
                attention_mask_response = attention_mask_full[:, -response_length:]
                
                # æ¯”è¾ƒç¬¬ä¸€ä¸ªæ ·æœ¬
                if sample_idx < info_mask_response.shape[0]:
                    info_sample = info_mask_response[sample_idx]
                    attn_sample = attention_mask_response[sample_idx]
                    mask_sample = response_mask[sample_idx]
                    
                    info_valid = info_sample.sum().item()
                    attn_valid = attn_sample.sum().item()
                    mask_valid = mask_sample.sum().item()
                    
                    print(f"  ğŸ“Š Mask comparison (Sample {sample_idx}):")
                    print(f"    info_mask valid tokens: {info_valid}")
                    print(f"    attention_mask valid tokens: {attn_valid}")
                    print(f"    response_mask valid tokens: {mask_valid}")
                    
                    if mask_source == "responses_types (multi-turn aware)":
                        if mask_valid > info_valid:
                            print(f"    âœ… response_mask uses responses_types - includes {mask_valid - info_valid} more assistant tokens from earlier turns")
                        elif mask_valid == info_valid:
                            print(f"    â„¹ï¸  response_mask matches info_mask (may indicate only last turn has assistant tokens)")
                        else:
                            print(f"    âš ï¸  Unexpected: response_mask has fewer tokens than info_mask")
                    elif info_valid != attn_valid:
                        print(f"    âš ï¸  Difference detected: info_mask excludes {attn_valid - info_valid} information tokens")
                        if mask_valid == info_valid:
                            print(f"    âœ… response_mask correctly uses info_mask")
                        elif mask_valid == attn_valid:
                            print(f"    âŒ response_mask incorrectly uses attention_mask (BUG!)")
                        else:
                            print(f"    âš ï¸  response_mask doesn't match either mask (unexpected)")
                    else:
                        print(f"    â„¹ï¸  info_mask and attention_mask are identical (no information blocks)")
        
        # æ£€æŸ¥å½¢çŠ¶åŒ¹é…
        if responses.shape == response_mask.shape:
            results["shapes_match"] = True
        elif verbose:
            print(f"  âŒ Shape mismatch: responses {responses.shape} vs mask {response_mask.shape}")
        
        # æ£€æŸ¥ mask æœ‰æ•ˆæ€§
        if results["shapes_match"]:
            mask_sum = response_mask.sum(dim=-1)  # (bs,)
            valid_lengths = mask_sum
            
            if verbose:
                print(f"  Valid tokens per sample: {valid_lengths.tolist()[:5]}...")
                print(f"  Mean valid ratio: {(mask_sum / responses.shape[1]).mean().item():.2%}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆçš„ maskï¼ˆå…¨0æˆ–å…¨1ä¸åˆç†ï¼‰
            if (mask_sum > 0).all() and (mask_sum < responses.shape[1]).any():
                results["mask_valid"] = True
            elif verbose:
                if (mask_sum == 0).any():
                    print(f"  âš ï¸  WARNING: Some samples have zero valid tokens!")
                if (mask_sum == responses.shape[1]).all():
                    print(f"  âš ï¸  WARNING: All tokens are marked as valid (unusual)")
        
        # æ£€æŸ¥æ ·æœ¬çº§åˆ«çš„å¯¹é½ - æ€»æ˜¯æ˜¾ç¤ºè‡³å°‘ä¸€ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
        if sample_idx < responses.shape[0] and results["shapes_match"]:
            resp_sample = responses[sample_idx]
            mask_sample = response_mask[sample_idx]
            valid_len = mask_sample.sum().item()
            
            # æ€»æ˜¯æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯ï¼ˆä¸ç®¡ verbose è®¾ç½®ï¼‰
            print(f"\nğŸ“‹ Sample {sample_idx} details:")
            print(f"  Response length: {len(resp_sample)}")
            print(f"  Valid tokens: {valid_len}")
            
            # æ˜¾ç¤ºå¤šè½®ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if "step_ids" in batch.batch and "responses_types" in batch.batch:
                step_ids_sample = batch.batch["step_ids"][sample_idx]
                responses_types_sample = batch.batch["responses_types"][sample_idx]
                
                # ç»Ÿè®¡æ¯ä¸ª step çš„æœ‰æ•ˆ token æ•°
                unique_steps = step_ids_sample.unique().tolist()
                print(f"\n  ğŸ”„ Multi-turn analysis:")
                print(f"    Total steps: {len(unique_steps)}")
                
                for step_id in unique_steps[:10]:  # æœ€å¤šæ˜¾ç¤ºå‰10ä¸ªstep
                    step_mask = (step_ids_sample == step_id) & (mask_sample > 0)
                    step_valid_count = step_mask.sum().item()
                    
                    if step_valid_count > 0:
                        # ç»Ÿè®¡è¿™ä¸ª step çš„ token ç±»å‹ï¼ˆåªç»Ÿè®¡æœ‰æ•ˆçš„ assistant tokensï¼Œæ’é™¤ informationï¼‰
                        # CRITICAL FIX: Only count tokens that are both in this step AND valid in response_mask
                        step_types = responses_types_sample[(step_ids_sample == step_id) & (mask_sample > 0)]
                        type_counts = {}
                        for rt in step_types.unique().tolist():
                            type_name = {0: 'think', 1: 'search', 2: 'answer', 
                                        3: 'information', 4: 'info_summary', 5: 'think_summary'}.get(rt, f'type_{rt}')
                            type_counts[type_name] = (step_types == rt).sum().item()
                        
                        type_str = ', '.join([f"{k}:{v}" for k, v in type_counts.items() if v > 0])
                        print(f"    Step {step_id}: {step_valid_count} valid tokens ({type_str})")
                
                if len(unique_steps) > 10:
                    print(f"    ... ({len(unique_steps) - 10} more steps)")
            
            # æ˜¾ç¤º mask æ¨¡å¼ï¼ˆæ€»æ˜¯æ˜¾ç¤ºï¼‰
            mask_list = mask_sample.tolist()
            first_valid_idx = next((i for i, m in enumerate(mask_list) if m > 0), None)
            last_valid_idx = next((i for i in range(len(mask_list)-1, -1, -1) if mask_list[i] > 0), None)
            if first_valid_idx is not None and last_valid_idx is not None:
                print(f"\n  ğŸ“Š Mask pattern:")
                print(f"    Valid token range: [{first_valid_idx}, {last_valid_idx}]")
                print(f"    Mask pattern (first 50): {''.join(['1' if m > 0 else '0' for m in mask_list[:50]])}")
                
                # æ˜¾ç¤º mask æ¨¡å¼ï¼ˆä¸­é—´æ®µï¼‰
                if len(mask_list) > 100:
                    mid_start = len(mask_list) // 2 - 25
                    mid_end = mid_start + 50
                    print(f"    Mask pattern (middle): {''.join(['1' if m > 0 else '0' for m in mask_list[mid_start:mid_end]])}")
                
                # æ˜¾ç¤º mask æ¨¡å¼ï¼ˆæœ«å°¾ï¼‰
                print(f"    Mask pattern (last 50): {''.join(['1' if m > 0 else '0' for m in mask_list[-50:]])}")
            
            # æ˜¾ç¤ºå®é™…å“åº”æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ tokenizerï¼Œæ€»æ˜¯æ˜¾ç¤ºï¼‰
            if tokenizer is not None:
                try:
                    # åªè§£ç æœ‰æ•ˆ token
                    valid_tokens = resp_sample[mask_sample.bool() if mask_sample.dtype == torch.bool else mask_sample > 0]
                    if len(valid_tokens) > 0:
                        decoded_text = tokenizer.decode(valid_tokens.tolist(), skip_special_tokens=False)
                        print(f"\n  ğŸ“ Decoded response (valid tokens only, {len(valid_tokens)} tokens):")
                        print(f"     {decoded_text}")
                    
                    # å¦‚æœ verboseï¼Œæ˜¾ç¤ºæ›´å¤šä¿¡æ¯
                    if verbose:
                        # æ˜¾ç¤ºå®Œæ•´åºåˆ—ï¼ˆåŒ…æ‹¬æ— æ•ˆéƒ¨åˆ†ï¼‰
                        full_decoded = tokenizer.decode(resp_sample.tolist(), skip_special_tokens=False)
                        print(f"\n  ğŸ“ Decoded full sequence:")
                        print(f"     {full_decoded[:500]}{'...' if len(full_decoded) > 500 else ''}")
                        
                        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªæœ‰æ•ˆ token
                        if len(valid_tokens) > 0:
                            print(f"\n  First 10 valid tokens: {valid_tokens[:10].tolist()}")
                            print(f"  Last 10 valid tokens: {valid_tokens[-10:].tolist()}")
                except Exception as e:
                    print(f"  âš ï¸  Failed to decode: {e}")
                    import traceback
                    traceback.print_exc()
            
            if verbose:
                # åªåœ¨ verbose æ¨¡å¼ä¸‹æ˜¾ç¤ºåŸå§‹ token IDs
                print(f"\n  First 20 tokens: {resp_sample[:20].tolist()}")
                print(f"  First 20 mask: {mask_sample[:20].tolist()}")
                
                # æ£€æŸ¥ mask æ˜¯å¦ä¸º 0/1
                mask_unique = mask_sample.unique().tolist()
                if mask_sample.dtype == torch.bool or (set(mask_unique).issubset({0, 1})):
                    print(f"  âœ… Mask values are binary: {mask_unique}")
                else:
                    print(f"  âš ï¸  WARNING: Mask contains non-binary values: {mask_unique}")
    
    elif "attention_mask" in batch.batch:
        # å°è¯•ä» attention_mask æ¨æ–­ response_mask
        attention_mask = batch.batch["attention_mask"]
        response_length = responses.shape[1]
        
        # å¼ºåˆ¶è¾“å‡º
        print(f"\nğŸ“Š Inferring response_mask from attention_mask:")
        print(f"  Attention mask shape: {attention_mask.shape}")
        print(f"  Response length: {response_length}")
        
        if attention_mask.shape[1] >= response_length:
            inferred_mask = attention_mask[:, -response_length:]
            print(f"  âœ… Inferred mask shape: {inferred_mask.shape}")
            try:
                print(f"  Valid tokens: {inferred_mask.sum(dim=-1).tolist()[:5]}...")
            except Exception as e:
                print(f"  âš ï¸  Failed to compute valid tokens: {e}")
    else:
        print(f"\nâš ï¸  Neither response_mask nor attention_mask found in batch")
    
    # ç»¼åˆç»“æœ
    results["all_passed"] = all([
        results["responses_exists"],
        results["response_mask_exists"] or "attention_mask" in batch.batch,
        results["shapes_match"] if results["response_mask_exists"] else True,
        results["mask_valid"] if results["shapes_match"] else True,
        results["responses_in_range"],
    ])
    
    # å¼ºåˆ¶è¾“å‡ºç»“æœï¼ˆå³ä½¿ verbose=Falseï¼‰
    print(f"\n{'âœ…' if results['all_passed'] else 'âŒ'} Overall check: {'PASSED' if results['all_passed'] else 'FAILED'}")
    print(f"  Responses exists: {results['responses_exists']}")
    print(f"  Response mask exists: {results['response_mask_exists']}")
    print(f"  Shapes match: {results['shapes_match']}")
    print(f"  Mask valid: {results['mask_valid']}")
    print(f"  Responses in range: {results['responses_in_range']}")
    
    return results


def check_responses_in_training(
    batch: DataProto,
    step: int = 0,
    check_freq: int = 10,
    verbose: bool = True,
    tokenizer=None
) -> bool:
    """
    åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        batch: æ•°æ®æ‰¹æ¬¡
        step: å½“å‰è®­ç»ƒæ­¥æ•°
        check_freq: æ£€æŸ¥é¢‘ç‡
        verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        tokenizer: Tokenizer for decoding (optional)
        
    Returns:
        True if checks passed
    """
    if check_freq <= 0 or (step % check_freq != 0):
        return True
    
    try:
        # å¼ºåˆ¶è¾“å‡ºæ£€æŸ¥å¼€å§‹æ ‡è®°
        print(f"\n{'='*80}")
        print(f"[RESPONSES CHECK] Step {step}")
        print(f"{'='*80}")
        
        results = check_responses_and_mask(batch, sample_idx=0, verbose=verbose, tokenizer=tokenizer)
        
        # å¼ºåˆ¶è¾“å‡ºæ£€æŸ¥ç»“æœ
        print(f"{'='*80}\n")
        
        return results["all_passed"]
    except Exception as e:
        print(f"âš ï¸  [RESPONSES CHECK] Step {step} FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

